# Real_Time_Twitter_Sentiment_Analysis



ğŸš€ Real-Time Twitter Sentiment Analysis Using Kafka, Spark, Scala, MongoDB 

This Big Data project performs real-time sentiment analysis on tweets using a complete data engineering and machine learning pipeline. It combines Kafka for data ingestion, Spark (Python + Scala) for stream and batch processing, Agentic AI for sentiment classification, MongoDB for storage.



ğŸ§  Overview

This project demonstrates how to build a real-time Big Data application that:
	â€¢	Ingests tweets via Kafka
	â€¢	Processes them using Spark Streaming
	â€¢	Classifies sentiment using an LLM (Agno/Groq) or MLlib
	â€¢	Stores results in MongoDB
	â€¢	Aggregates trends using Scala + Spark

ğŸ“¡ It also integrates Scala for batch analytics and Hadoop (HDFS) to store summaries, fulfilling distributed system requirements.



ğŸ“ Project Architecture

[Twitter API / CSV Simulator]
         â†“
     Kafka Producer
         â†“
    [Kafka Topic]
         â†“
  Kafka Consumer (Python)
         â†“
  Agentic AI (Agno + Groq)
         â†“
       MongoDB
     â†™         
Scala Spark  
(HDFS Output)



ğŸ”§ Technologies Used

Layer	Tech Stack
Ingestion	Apache Kafka, Docker
Real-Time Compute	PySpark (Streaming), Agentic AI (Agno)
Batch Analytics	Scala + Spark + HDFS
Model Training	MLlib (Logistic Regression on PySpark)
Storage	MongoDB
Deployment	Docker (Kafka Zookeeper setup)




ğŸ—ƒ Dataset Used
	â€¢	ğŸ“ twitter_training.csv: 74,682 labeled tweets for training
	â€¢	ğŸ“ twitter_validation.csv: 998 tweets for validation
	â€¢	Source: Twitter Entity Sentiment Analysis - Kaggle



ğŸ“¦ Repository Structure

.
â”œâ”€â”€ Kafka-PySpark/
â”‚   â”œâ”€â”€ kafka_producer.py        # Sends tweets to Kafka topic
â”‚   â”œâ”€â”€ kafka_consumer.py        # Consumes, classifies, and stores to MongoDB
â”œâ”€â”€ ML PySpark Model/
â”‚   â”œâ”€â”€ model_training.ipynb     # PySpark MLlib logistic regression
â”‚   â”œâ”€â”€ twitter_training.csv
â”‚   â”œâ”€â”€ twitter_validation.csv
â”œâ”€â”€ Scala-Spark-Batch/
â”‚   â””â”€â”€ sentiment_summary.scala  # Scala job to aggregate MongoDB â†’ HDFS
â”œâ”€â”€ zk-single-kafka-single.yml   # Kafka + Zookeeper Docker setup
â”œâ”€â”€ run_kafka_pipeline.py        # Launches producer + consumer together
â””â”€â”€ bigdataproject_rapport.pdf   # French summary report (if required)




âš™ï¸ Setup & Run

1. ğŸš€ Start Kafka (Docker)

docker-compose -f zk-single-kafka-single.yml up -d

2. ğŸ§  Train PySpark MLlib Model (optional)

Run model_training.ipynb to train and save your model.

3. ğŸ›°ï¸ Start Producer + Consumer

python3 run_kafka_pipeline.py

4. ğŸ“¦ Run Scala Spark Batch (optional)

spark-submit --class SentimentSummary --master local Scala-Spark-Batch/sentiment_summary.scala




ğŸ“ˆ Features
	â€¢	âœ… Live tweet ingestion using Kafka
	â€¢	âœ… Real-time sentiment classification with LLM
	â€¢	âœ… MongoDB storage of tweets and sentiments
	â€¢	âœ… Scala-based trend aggregation saved to HDFS
	â€¢	âœ… End-to-end Big Data pipeline



ğŸ“Œ Contribution Ideas
	â€¢	Add toxicity or emotion classification
	â€¢	Train your own ML model with PySpark or Scala
	â€¢	Integrate with Elasticsearch for tweet search
	â€¢	Add REST API endpoints for dashboard data



